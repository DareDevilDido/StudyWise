Hi. In this video we are going to see how we can perform speech to text using deep speech package. Now before going into deep speech package, what is speech to text? Speech to text basically takes an audio data and converts it into a machine readable form. So basically it takes an audio data as an input and gives an text data as an output. Now deep speech is used to accomplish speech to text and it an open source voice recognization or speech to text plus system that uses a neural network to convert speech spectrograms into text transcripts. Deep speech is based on Baidu's open source research paper. You can find the research paper online with the same name called deep speech and there are a lot of technical details in it. Now this deep speech is developed using a single end to end deep learning. Deep learning framework, right? So there are two models. Typically in a speech to text system there is an acoustic model and there is an language model. The acoustic model is an end to end deep learning system. And there's a language model which is kind of used to increase the accuracy of the transcription output, which is a separate model. And the language model is something you can customize to your domain as well. So let's get started and I'll explain more in details as we get into that part. I'm installing deep speech. I'm installing 8.2 version which is the latest table release. The 0.9 is an alpha release. So I'm using the latest table release and I'm downloading two files from again the deep speech repo. One is the PBMM file and the scorer file. Now this PBMM file is the acoustic model and it is trained on American English. So if you want to kind of use this model for an UK English or kind of Asian English, then maybe you may have to collect data and fine tune it or train it from scratch for your purpose depending on the data size. But this is developed on American English. But UK English typically work with this model to most extent, but the accuracy may be lower than what an American English will be. The second thing I am downloading is, and again this PvmM file is a memory mapped file and thus it is very memory efficient and fast to load for inferences. There is also a lighter version in the back end uses tensorflow. There is a TF lite version as well in the repo which you can download it and it also supports post quantization training so that you can enable in low power and low consumption devices. The next part is the scorer. The scorer is the language model. And the purpose of the language model is typically improve the accuracy of the predicted output. So whatever your acoustic model gives you, it does not understand the semantics. It is kind of taking and converting it as the user is speaking. And you may want to sometimes go back and see grammatically which word is right. And typically a language model runs through multiple probabilities and pick the right word in that particular context. That's what language model does. So I'm downloading these two, I have already downloaded it because the files are a little bigger and in size it takes time. So I've already downloaded it. And also what I'm doing is I am kind of downloading some sample audio files, which is again the deep space to test the model. And you can see like basically I downloaded it, I have untard it and that's the audio directory which I'm printing it. And here you can basically see these are all some of the sample files that are available. I'm going to use files outside of this as well, but we will try with the sample files as well. Now let me do a quick list of file system and you can see there is an PBMM file and there's a scorer file. So let me start importing the packages. So I am importing from deep speech model which is to create the model. Let me run the next two step and then talk about it. And then I'm importing numpy. Numpy is basically I want to buffer the incoming audio file, hand feed it for inference. I am downloading OS and wave file. Wave file is basically to read the audio file and then I am having an ipython display import audio. So this is just to quickly play the audio in hand Jupyter browser. And that's why I'm importing it. I am having the model file path. I am having the basically language model file path. The model file path is nothing but the acoustic model. That is a PBMM file. And the language file path is the scorer which I showed you on the top I am setting some other parameters. The alpha and beta are used for a language model. Basically it is there in the website. I have just taken it and I am giving it as it is. So this is based on the training. You can change the parameter and see what works for your data better. But if you go to the GitHub repo over there, you can see these are all the optimal parameter that was used, that the model worked best to do the transcription. So I am just using these two from that website. You can again, change it and then you can use it as well. See what works for data. The one other parameter is beam width. So beam width is basically, the idea is like how many different word sequences that needs to be evaluated by your acoustic model. And that's what beam width is. The more you give beam width, the more better the acoustic model conversion will be. But it comes with the cost as well. So you need to kind of balance it and see what works better. Typically the 500 range and all, everything works better. But if you want even faster inference, you want to basically make it to lower like 100 or 200. And the language model the scorer. Now typically you will have domain specific words like if you are in a banking, you may have investment account, debit account, credit account. If you are in healthcare, you may have like drug names and everything. So basically this model was developed on open source data and might not recognize it. You may want to custom train a language model and then feed it so that it can basically understand your business vocabulary better. And that's where the wrong language model is. So I run this two step. What I'm doing is the alpha, beta and beam with I am using c. In this case I am taking the model and loading the model file path, the acoustic model. And once I have the model object, I'm setting the language model as well, right? So when an audio file comes in, it goes to the acoustic model. The acoustic model converts the speech spectrogram into text transcript, but the output may not be perfect. And language model refines the output to get a better text prediction output. And then I am setting the alpha, beta and beam width that I have set on the top. Now I'm going to create two functions. What I'm doing, the first function I am creating is basically I am taking a file name, a wave file name. I am opening the wave file name. I am getting the frame rate. So what is the frame rate for this particular audio file? The libre speech is the open source data set on which this was trained on. And it is like 16 khz frame rate. So my audio also has to be similar to that. If it's higher, it may not work. So there are utilities that are available that can reduce the frame rate of your audio file. So you can use that as a preprocessing step. And then I am checking how many frames are there in this particular audio file. Now there are two modes that you can run. You can run in a batch mode and a streaming mode. The streaming mode I will cover in the later video. Basically batch mode is you take the entire audio file and do the transcription and one shot. But sometimes you may have an audio file that is kind of playing for 20 minutes and you don't want to wait for the process to complete. So what you can do is you can read the audio file part by part. You can buffer the audio frames part by part and then print the output as the conversion is happening. That is a streaming mode. And typically when you are using a microphone to speak, you want to use the streaming mode. So as you are speaking it may be to collect the data and then you are transcribing it. Right. And then I'm getting number of frames in the audio file. In this case I am telling read all the frames. If you see, basically it's going to read all the frames and I'm returning the buffer and rate over here. That is the first function I have. So this is just to read the file and get the final audio bytes. The buffer will give me the final audio bytes that I am, that the second is I am called the transcription function. Right? In this transcription I am again taking an audio file. And if you see the very first thing I'm doing is I'm calling this other function and getting the buffer and rate. And then I'm using the numpy from buffer. I am loading. The buffer is nothing but the audio bytes data. I am passing the bytes data and then I'm using the model, finally the model that I created on the top and I'm calling model STT speech to text of this particular data. So I have the bytes data in my numpy buffer. I am passing it and finally I'll get the transcript output in this stage. Let me download a simple file to test how it is. So this is the file I have. I have downloaded it. It has a speech in it. Let me quickly check that. There is one file, one more file I downloaded in the previous stages, basically like an audio file that comes with libe speech and that is this audio file. So let me quickly play this audio. So this is the audio file. I'm using the ipython audio. Let me first play this. Why should one on the way, basically you can basically see now like it's kind of playing the output. And I have one more audio file that I recently downloaded. And this is what this is saying, right. This audio file, if I go and play this, you can basically see very different this audio file. This is the word. So now I'm going to call transcribe function in both of this. So in this transcribe function what I'm going to do is I'm going to call this particular first audio file that I will listen to. And in this case it's printing the bytes I gave the print in the function and printing the frames. And why should one all on the way, right? So this is the word, there is like some error, but that's fine. Species X will not be accurate. You can fine tune it, right? I will come to that in my future video. The next is I am taking the wave file and then I am converting it. That is the second file I downloaded. Now, in this case, if you see, it's telling that administer medicine to animal is frequently a very difficult matter, and yet sometimes it's necessary to do so. And if you have heard this particular wave file, basically animals, that's what it is telling as well. Sometimes it's necessary to do so. So this is one, I'm going to download one more file from the same content over here and then I am going to play the audio and then I am going to transcribe. Now, this is shorter file, that's why it's able to run faster. But longer files will take time and that's where I need to maybe use the streaming API. Let me run this as well in parallel. And now if you see whatever was transcribed on the top, it's going to take it and it's going to transcribe. In the course of a December tour in auction, I load for a long distance and that was the content of the file as well. Right. Now, real world file may have a lot of background noise, there may be some other cross talk and everything. So in that case, we may want to train our model further so that it is able to accommodate for the cross talk and accommodate for the backward noise. And those are all like challenges where you can go and even train your acoustic model if you have a lot of data, say in the company, if you are running a call center operation, if you have a lot of data, you want to go and train from scratch, you can do that as well, or you can even fine tune the exist model. So that's about it. Thank you very much, much.